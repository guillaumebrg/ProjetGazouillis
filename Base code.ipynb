{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATASET = np.load(\"data/dataset.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape : (5520598L, 160L, 1L)\n"
     ]
    }
   ],
   "source": [
    "print \"Shape :\", DATASET.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into a trainset, a validset and a testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_dataset(dataset, split_rate=[0.8, 0.1, 0.1], seed=123): # see preprocessing.py\n",
    "    # index = 1,2,3,...,N\n",
    "    N = dataset.shape[0]\n",
    "    index = range(dataset.shape[0])\n",
    "    # shuffle\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(index)\n",
    "    # division\n",
    "    train_rate, valid_rate, test_rate = np.cumsum(split_rate)\n",
    "    index_train = index[0:int(train_rate*N)]\n",
    "    index_valid = index[int(train_rate*N):int(valid_rate*N)]\n",
    "    index_test = index[int(valid_rate*N):int(test_rate*N)]\n",
    "    # return \n",
    "    return index_train, index_valid, index_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_train, index_valid, index_test = split_dataset(DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**index_train, index_valid, index_test** are such that :\n",
    "\n",
    "trainset = DATASET[index_train]\n",
    "\n",
    "validset = DATASET[index_valid]\n",
    "\n",
    "testset = DATASET[index_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ACCEPTED_CHARS = params.ACCEPTED_CHARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '#', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\x80', '\\x92', '\\x98', '\\x9f', '\\xa6', '\\xe2', '\\xf0']\n"
     ]
    }
   ],
   "source": [
    "print ACCEPTED_CHARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch2onehot(batch, D): # see preprocessing.py\n",
    "    ''' Function used during the training to encode batches.\n",
    "    Input size : (batch_size, tweet_length, 1).\n",
    "    Output size : (batch_size, tweet_length, D)'''\n",
    "    B, T = batch.shape[0:2]\n",
    "    one_hot_batch = np.zeros((B*T, D))\n",
    "    one_hot_batch[range(B*T), batch.flatten()] = 1\n",
    "    one_hot_batch = one_hot_batch.reshape((B,T,D))\n",
    "    return one_hot_batch\n",
    "\n",
    "def batch2tweet(batch, accepted_caracters, special_char=\"\"): # see preprocessing.py\n",
    "    '''Not optimized. But not used during the training : no need to be fast.'''\n",
    "    tweets = []\n",
    "    for t in batch:\n",
    "        tweet = \"\"\n",
    "        for char in t:\n",
    "            try:\n",
    "                tweet += accepted_caracters[char[0]]\n",
    "            except:\n",
    "                tweet += special_char # Special marker indicating the end of the tweet\n",
    "        tweets.append(tweet)\n",
    "    return tweets\n",
    "\n",
    "def onehot2tweet(batch, accepted_caracters, special_char=\"\"): # see preprocessing.py\n",
    "    '''Not optimized. But not used during the training : no need to be fast.'''\n",
    "    tweets = []\n",
    "    for t in batch:\n",
    "        tweet = \"\"\n",
    "        for char in t:\n",
    "            try:\n",
    "                tweet += accepted_caracters[np.where(char==1)[0][0]]\n",
    "            except:\n",
    "                tweet += special_char # Special marker indicating the end of the tweet\n",
    "        tweets.append(tweet)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char 76 from tweet 40 : [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "D = params.D  # dimension of one-hot vectors\n",
    "B = params.B  # batch size for the demo\n",
    "T = params.T  # max length of a tweet\n",
    "batch = DATASET[0:B] \n",
    "# Dataset > One-hot\n",
    "one_hot_batch = batch2onehot(batch, D)\n",
    "\n",
    "# Pick a tweet\n",
    "t = np.random.randint(B)\n",
    "# Pick a char\n",
    "c = np.random.randint(T)\n",
    "print \"Char %d from tweet %d :\"%(c,t), one_hot_batch[t][c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you want to, possible to go back in the \"string\" domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rt @kendrakuuddy: i pray to god every day not to ask him for more but to thank him for everything he already has blessed me with ��\n",
      "rt @kendrakuuddy: i pray to god every day not to ask him for more but to thank him for everything he already has blessed me with ��\n"
     ]
    }
   ],
   "source": [
    "# Using 'batch2tweet' :\n",
    "t = np.random.randint(B) # Pick a tweet\n",
    "print batch2tweet(batch, ACCEPTED_CHARS)[t]\n",
    "# or 'onehot2tweet' if the tweet has already been converted to onehot :\n",
    "print onehot2tweet(one_hot_batch, ACCEPTED_CHARS)[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_generator(data, index, batch_size, D): # see preprocessing.py\n",
    "    # Init iterator and shuffling the dataset\n",
    "    count = 0\n",
    "    np.random.shuffle(index)\n",
    "    while 1:\n",
    "        if count+batch_size >= len(index):\n",
    "            # Reset counter and shuffling\n",
    "            count = 0\n",
    "            np.random.shuffle(index)\n",
    "        # Get raw data\n",
    "        raw_batch = np.copy(data[index[count:(count+batch_size)]])\n",
    "        # One-hot encoding\n",
    "        one_hot_batch = batch2onehot(raw_batch, D)\n",
    "        # Remove the padding dimension\n",
    "        one_hot_batch = one_hot_batch[:,:,0:(D-1)] # s.t. padding features are full of 0s\n",
    "                                                   # and will be masked by the Masking layer\n",
    "                                                   # (see below in the model definition)\n",
    "        # Target\n",
    "        input_batch  = one_hot_batch[:, 0:-1, :]\n",
    "        target_batch = one_hot_batch[:, 1:, :] # target = 1-shifted input batch\n",
    "        del raw_batch\n",
    "        count += batch_size\n",
    "        # Yield\n",
    "        yield input_batch, target_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom functions used to save stuff..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**save_architecture** : save the architecture of the model in a text file.\n",
    "\n",
    "**create_log** : create a text file where the loss, and other metrics will be printed using **write_log**\n",
    "\n",
    "**ModelSaver** : Custom Keras callback (https://keras.io/callbacks/). This object is given to the 'fit' (or equivalently 'fit_generator') when launching a training. It permits to control when to save weights, and underwhich name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 850M (CNMeM is disabled, cuDNN 5005)\n"
     ]
    }
   ],
   "source": [
    "def save_architecture(model, path_out): # see logging.py\n",
    "    \"\"\"\n",
    "    Based on the keras utils 'model.summary()'\n",
    "    \"\"\"\n",
    "    # Redirect the print output the a textfile\n",
    "    orig_stdout = sys.stdout\n",
    "    # and store the architecture\n",
    "    f = file(os.path.join(path_out, \"architecture.txt\"), 'w')\n",
    "    sys.stdout = f\n",
    "    model.summary()\n",
    "    # Reset the print output direction\n",
    "    sys.stdout = orig_stdout\n",
    "    f.close()\n",
    "\n",
    "    open(os.path.join(path_out, \"config.json\"), 'w').write(model.to_json())\n",
    "\n",
    "\n",
    "def create_log(path, settings, filename=\"log.txt\"): # see logging.py\n",
    "    f = open(os.path.join(path, filename), \"w\")\n",
    "    f.writelines(str(settings))\n",
    "    f.writelines(\"\\n####\\nStarted on %s at %s\\n\" % (time.strftime(\"%d/%m/%Y\"), time.strftime(\"%H:%M:%S\")))\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def write_log(path, string, filename=\"log.txt\"): # see logging.py\n",
    "    \"\"\"\n",
    "    Add a line at the end of a textfile.\n",
    "\n",
    "    :param path: textfile location\n",
    "    :param string: line to add\n",
    "    \"\"\"\n",
    "    # Open and Read\n",
    "    f = open(os.path.join(path, filename), \"r\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    # Adding a line\n",
    "    lines.append(string)\n",
    "    # Write\n",
    "    f = open(os.path.join(path, filename), \"w\")\n",
    "    f.writelines(lines)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "    \n",
    "class ModelSaver(Callback): # see logging.py\n",
    "    \"\"\"\n",
    "    Keras callback subclass which defines a saving procedure of the model being trained : after each epoch,\n",
    "    the last model is saved under the name 'after_random.cnn'. The best model is saved with the name 'best_model.cnn'.\n",
    "    The model after random can also be saved. And the model architecture is saved with the name 'config.network'.\n",
    "    Everything is stored using pickle.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, path_weights, monitor, verbose=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.verbose = verbose\n",
    "        self.path = path\n",
    "        self.path_weights = path_weights\n",
    "        self.monitor = monitor\n",
    "        self.best = np.Inf\n",
    "        \n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.epoch_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.epoch_end = time.time()\n",
    "        # get loss\n",
    "        monitor = logs.get(self.monitor)\n",
    "        # condition = True if loss decreased\n",
    "        condition = monitor < self.best\n",
    "\n",
    "        if condition:\n",
    "            # Save weights as \"best_model.weights\"\n",
    "            self.best = monitor\n",
    "            save_path = os.path.join(self.path_weights, \"best_model.weights\")\n",
    "            self.model.save_weights(save_path, overwrite=True)\n",
    "        else:\n",
    "            # Save weights as \"last_epoch.weights\"\n",
    "            save_path = os.path.join(self.path_weights, \"last_epoch.weights\")\n",
    "            self.model.save_weights(save_path, overwrite=True)\n",
    "        \n",
    "        # Log file management\n",
    "        if self.verbose > 0:\n",
    "            log_string = \"####\\nEpoch %d took %d s: \" % (epoch, int(self.epoch_end-self.epoch_start))\n",
    "            for k in logs.keys():\n",
    "                log_string += \"%s : %.4f # \" % (k, logs.get(k))\n",
    "            if condition:\n",
    "                log_string += \"\\tBEST\"\n",
    "            write_log(self.path, log_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Masking, Dropout, TimeDistributed, Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_model(T, D, lr, nhidden, drop_rate): # see models.py\n",
    "    # Input layer\n",
    "    inputs = Input((T, D))\n",
    "    # Masking \"only-0\" input features\n",
    "    masked = Masking(mask_value=0.0)(inputs)\n",
    "    # Hidden layers\n",
    "    for i in range(nhidden):\n",
    "        if i == 0:\n",
    "            hidden  = LSTM(128, return_sequences=True)(masked)\n",
    "        else:\n",
    "            hidden  = LSTM(128, return_sequences=True)(dropout)\n",
    "        dropout = Dropout(drop_rate)(hidden)\n",
    "    # Output layer : linear TimeDistributedDense + softmax\n",
    "    decoder = TimeDistributed(Dense(D))(dropout) # Apply the same dense layer on each timestep\n",
    "    outputs = Activation(\"softmax\") (decoder)\n",
    "\n",
    "    model = Model(input=inputs, output=outputs)\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=lr), loss=\"categorical_crossentropy\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model instanciation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LR = params.LR # learning rate\n",
    "model = get_model(T-1, D-1, LR, 3, 0.1) # D-1 because params.D accounts for the padding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 159, 63)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_1 (Masking)              (None, 159, 63)       0           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 159, 128)      98304       masking_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 159, 128)      0           lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                    (None, 159, 128)      131584      dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 159, 128)      0           lstm_2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                    (None, 159, 128)      131584      dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 159, 128)      0           lstm_3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_1 (TimeDistribute(None, 159, 63)       8127        dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 159, 63)       0           timedistributed_1[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 369599\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some test on random inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_sequence = np.random.randn(T*(D-1)).reshape((1, T, D-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1L, 160L, 63L)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(input_sequence).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PATH_EXPERIMENT = params.PATH_EXPERIMENT\n",
    "NB_EPOCHS = params.NB_EPOCHS\n",
    "NB_SAMPLES_PER_EPOCH = params.NB_SAMPLES_PER_EPOCH\n",
    "PATIENCE = params.PATIENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainargs2strings(path, model, dataset, index_train, index_valid, D, batch_size, \n",
    "             nsamples_per_epoch, nepoch, patience): # see logging.py\n",
    "    settings = \"\"\n",
    "    settings += \"Path : %s\"%path\n",
    "    settings += \"\\nDataset shape :\" + str(dataset.shape)\n",
    "    settings += \"\\nNtrain : %d\"%len(index_train)\n",
    "    settings += \"\\nNvalid : %d\"%len(index_valid)\n",
    "    settings += \"\\nBatch size : %d\"%batch_size\n",
    "    settings += \"\\nNb samples per epoch : %d\"%nsamples_per_epoch\n",
    "    settings += \"\\nNb epochs : %d\"%nepoch\n",
    "    settings += \"\\nPatience : %d\"%patience\n",
    "    return settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def training(path, model, dataset, index_train, index_valid, D, batch_size, \n",
    "             nsamples_per_epoch, nepoch, patience): # see training.py\n",
    "    start = time.time()\n",
    "    # Create dir (if not already done)\n",
    "    if os.path.exists(path) is False:\n",
    "        os.mkdir(os.path.abspath(path))\n",
    "    path_weights = os.path.join(path, \"weights\")\n",
    "    if os.path.exists(path_weights) is False:\n",
    "        os.mkdir(os.path.abspath(path_weights))\n",
    "    # Create log file\n",
    "    settings = trainargs2strings(path, model, dataset, index_train, index_valid, D, batch_size, \n",
    "                                 nsamples_per_epoch, nepoch, patience)\n",
    "    create_log(path, settings)\n",
    "    # Save architecture\n",
    "    save_architecture(model, path)\n",
    "    # Save weights after initialization\n",
    "    model.save_weights(os.path.join(path_weights, \"after_initialization.weights\"), \n",
    "                       overwrite=True)\n",
    "\n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\", patience=patience)\n",
    "    model_saver = ModelSaver(path, os.path.join(path, \"weights\"), monitor=\"val_loss\")\n",
    "    # Argument to give to generators\n",
    "    train_generator_args = [dataset, index_train, batch_size, D]\n",
    "    valid_generator_args = [dataset, index_valid, batch_size, D]\n",
    "    # Training loop\n",
    "    h = model.fit_generator(batch_generator(*train_generator_args), nsamples_per_epoch, nepoch, \n",
    "                            validation_data=batch_generator(*valid_generator_args), \n",
    "                            nb_val_samples=len(index_valid),\n",
    "                            callbacks=[early_stopping, model_saver])\n",
    "\n",
    "    end = time.time()\n",
    "    print \"Training took %.2fs\"%(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-f38dc29a1888>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpatience\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;31m# patience\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m training(PATH_EXPERIMENT, model, DATASET, index_train, index_valid[0:(b*50)], D, \n\u001b[0m\u001b[0;32m      8\u001b[0m          b, n, nepoch, patience)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'training' is not defined"
     ]
    }
   ],
   "source": [
    "# Quick test\n",
    "b = 256 # batch size\n",
    "n = b*50 # number of examples in 1 epoch\n",
    "nepoch = 3 # number of epochs\n",
    "patience = 3 # patience\n",
    "\n",
    "training(PATH_EXPERIMENT, model, DATASET, index_train, index_valid[0:(b*50)], D, \n",
    "         b, n, nepoch, patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Needs to be filled - Sampling functions - \"Temperature\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"best_model.weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# n o w p l a y i n g "
     ]
    }
   ],
   "source": [
    "first_character = \"#\"\n",
    "i = np.where(np.array(ACCEPTED_CHARS)==first_character)[0]\n",
    "\n",
    "sequence = np.zeros((1, T-1, D-1), \"float32\")\n",
    "sequence[0,0,i] = 1\n",
    "\n",
    "print first_character,\n",
    "for t in range(T-2):\n",
    "    out = model.predict(sequence)   \n",
    "    next_char = np.random.choice(range(D-1), p=out[0,t]/out[0,t].sum())\n",
    "    sequence[0,t+1,next_char] = 1\n",
    "    if next_char != 62:\n",
    "        char = ACCEPTED_CHARS[next_char]\n",
    "    else:\n",
    "        break\n",
    "    print char,\n",
    "    \n",
    "onehot2tweet(sequence, ACCEPTED_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
