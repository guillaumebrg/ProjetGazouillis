{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import nltk\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape : (6126219, 161, 1)\n"
     ]
    }
   ],
   "source": [
    "DATASET = np.load(\"data/dataset.npy\")\n",
    "print \"Shape :\", DATASET.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch2onehot(batch, D): # see preprocessing.py\n",
    "    ''' Function used during the training to encode batches.\n",
    "    Input size : (batch_size, tweet_length, 1).\n",
    "    Output size : (batch_size, tweet_length, D)'''\n",
    "    B, T = batch.shape[0:2]\n",
    "    one_hot_batch = np.zeros((B*T, D))\n",
    "    one_hot_batch[range(B*T), batch.flatten()] = 1\n",
    "    one_hot_batch = one_hot_batch.reshape((B,T,D))\n",
    "    return one_hot_batch\n",
    "\n",
    "def batch2tweet(batch, accepted_caracters, special_char=\"\"): # see preprocessing.py\n",
    "    '''Not optimized. But not used during the training : no need to be fast.'''\n",
    "    tweets = []\n",
    "    for t in batch:\n",
    "        tweet = \"\"\n",
    "        for char in t:\n",
    "            try:\n",
    "                tweet += accepted_caracters[char[0]]\n",
    "            except:\n",
    "                tweet += special_char # Special marker indicating the end of the tweet\n",
    "        tweets.append(tweet)\n",
    "    return tweets\n",
    "\n",
    "def onehot2tweet(batch, accepted_caracters, special_char=\"\"): # see preprocessing.py\n",
    "    '''Not optimized. But not used during the training : no need to be fast.'''\n",
    "    tweets = []\n",
    "    for t in batch:\n",
    "        tweet = \"\"\n",
    "        for char in t:\n",
    "            try:\n",
    "                tweet += accepted_caracters[np.where(char==1)[0][0]]\n",
    "            except:\n",
    "                tweet += special_char # Special marker indicating the end of the tweet\n",
    "        tweets.append(tweet)\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Masking, Dropout, TimeDistributed, Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_truncated2(T, D, lr, nhidden, drop_rate, nb_neuron_hidden): # see models.py\n",
    "    # Input layer\n",
    "    inputs = Input((T, D))\n",
    "    # Masking \"only-0\" input features\n",
    "    masked = Masking(mask_value=0.0)(inputs)\n",
    "    # Hidden layers\n",
    "    for i in range(nhidden):\n",
    "        if i == (nhidden-1):\n",
    "            outputs = LSTM(nb_neuron_hidden, return_sequences=True)(dropout)\n",
    "        elif i == 0:\n",
    "            hidden  = LSTM(nb_neuron_hidden, return_sequences=True)(masked)\n",
    "        else:\n",
    "            hidden  = LSTM(nb_neuron_hidden, return_sequences=True)(dropout)\n",
    "        dropout = Dropout(drop_rate)(hidden)\n",
    "\n",
    "    model = Model(input=inputs, output=outputs)\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=lr), loss=\"categorical_crossentropy\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = 161\n",
    "D = 64\n",
    "LR = params.LR # learning rate\n",
    "model = get_model_truncated2(T-1, D-1, LR, 2, 0.1, 512) # D-1 because params.D accounts for the padding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 160, 63)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_1 (Masking)              (None, 160, 63)       0           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 160, 512)      1179648     masking_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 160, 512)      0           lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                    (None, 160, 512)      2099200     dropout_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 3278848\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the weights\n",
    "weights = np.load('results/exp_014/weights/best_model.npy')\n",
    "#model.set_weights(weights)\n",
    "model.layers[2].set_weights(weights[0:12])\n",
    "model.layers[4].set_weights(weights[12:24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ACCEPTED_CHARACTERS = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', \n",
    "                       'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] + \\\n",
    "['%d'%i for i in range(10)] + \\\n",
    "['/', '@', '#', '&', '|', '|', '|', '|', '|', '|', '|'] + \\\n",
    "['\\n', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', ':', ';', '?', '_','|']\n",
    "\n",
    "ACCEPTED_CHARACTERS.sort()\n",
    "ACCEPTED_CHARACTERS.append('')\n",
    "accepted_characters = ACCEPTED_CHARACTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet2string(tweet, accepted_characters = ACCEPTED_CHARACTERS):\n",
    "    output = ''\n",
    "    for t in tweet:\n",
    "        output = output + accepted_characters[t[0]]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adapted from http://stackoverflow.com/questions/14776317/finding-exact-position-of-tokenized-sentences\n",
    "from nltk import tokenize\n",
    "from nltk import TweetTokenizer\n",
    "\n",
    "def token_position(tweet, tags):\n",
    "    tknzr = TweetTokenizer()\n",
    "    \n",
    "    output = []\n",
    "    offset, length, offset2,length = 0, 0, 0, 0\n",
    "    for i,sentence in enumerate(tknzr.tokenize(tweet)):\n",
    "        # fix ignored characters\n",
    "        offset = tweet.find(sentence, offset)\n",
    "        length = len(sentence)\n",
    "        \n",
    "        output.append([offset, length, tags[i]])\n",
    "        offset += length\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categorize_words(tweet):\n",
    "    tknzr = TweetTokenizer()\n",
    "\n",
    "    token = tknzr.tokenize(tweet)\n",
    "    tag = nltk.pos_tag(token)\n",
    "    tweet_tokenized = token_position(tweet, tag)\n",
    "    tweet_tokenized\n",
    "\n",
    "    word_position = 0\n",
    "    tags = []\n",
    "\n",
    "    for i, c in enumerate(tweet):\n",
    "\n",
    "        if i > (tweet_tokenized[word_position][0] + tweet_tokenized[word_position][1]):\n",
    "            word_position = word_position + 1\n",
    "\n",
    "        if i != (tweet_tokenized[word_position][0] + tweet_tokenized[word_position][1]) and i>=tweet_tokenized[word_position][0]:\n",
    "            tags.append(tweet_tokenized[word_position][2][1])\n",
    "        else:\n",
    "            tags.append('.')\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# categorize words\n",
    "n = 150\n",
    "tweets = DATASET[0:n]\n",
    "\n",
    "tweet_syntax = []\n",
    "for t in tweets:\n",
    "    tweet_str = tweet2string(t, ACCEPTED_CHARACTERS)\n",
    "    tweet_syntax.append(categorize_words(tweet_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 160, 512)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the model prediction\n",
    "one_hot_batch = batch2onehot(tweets, D)\n",
    "one_hot_batch = one_hot_batch[:,0:160,0:(D-1)]\n",
    "\n",
    "out = model.predict(one_hot_batch)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for all tweets, seperate the words and get their categories\n",
    "word = []\n",
    "word_syntax = []\n",
    "    \n",
    "for t in range(out.shape[0]):\n",
    "    tmp_word = []\n",
    "    on = False\n",
    "\n",
    "    last_c = ''\n",
    "    for i, c in enumerate(tweet_syntax[t]):\n",
    "        if last_c != c and on:\n",
    "            word.append(np.asarray(tmp_word))\n",
    "            word_syntax.append(last_c)\n",
    "            tmp_word = []\n",
    "            on = False\n",
    "        elif last_c != c and not on:\n",
    "            on = True\n",
    "\n",
    "        if c == '.':\n",
    "            on = False\n",
    "        else:\n",
    "            tmp_word.append(out[t][i])\n",
    "        last_c = c\n",
    "\n",
    "word = np.asarray(word)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get gram matrices for each word\n",
    "gram = []\n",
    "for w in word:\n",
    "    gram.append(np.dot(np.transpose(w),w))\n",
    "    \n",
    "gram = np.asarray(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2098, 512, 512)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(262144, 2098)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2 = np.reshape(gram, (gram.shape[0], (gram.shape[1]*gram.shape[2])))\n",
    "out2 = np.transpose(out2)\n",
    "out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PCA from 10000 to 100 dimensions\n",
    "pca = PCA(n_components = 100)\n",
    "pca.fit(out2)\n",
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "components = np.transpose(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne_model = TSNE(n_components=2, random_state = 0)\n",
    "data = tsne_model.fit_transform(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category = ['?']\n",
    "colors = []\n",
    "\n",
    "for w in word_syntax:\n",
    "    if w[0] not in category:\n",
    "        if w[0].isalpha():\n",
    "            category.append(w[0])\n",
    "            \n",
    "    if w[0].isalpha():\n",
    "        #>print w[0]\n",
    "        colors.append(category.index(w[0]))\n",
    "    else:\n",
    "        colors.append(category.index('?'))\n",
    "\n",
    "#colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colors = np.array(colors)\n",
    "\n",
    "\n",
    "for i in range(len(category)):\n",
    "    c = np.where(colors == i)\n",
    "    plt.scatter(data[c,0], data[c,1], c=np.random.rand(3,1), label = category[i])\n",
    "plt.legend(loc='center left', bbox_to_anchor = (1,0.5))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
